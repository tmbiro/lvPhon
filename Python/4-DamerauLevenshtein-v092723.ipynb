{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textdistance as td\n",
    "\n",
    "# Make sure you can see all output\n",
    "pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Production</th>\n",
       "      <th>Prod_Word_Dur</th>\n",
       "      <th>Prod_Arpabet</th>\n",
       "      <th>Prod_Phon_Dur</th>\n",
       "      <th>NOTES</th>\n",
       "      <th>Word_ID</th>\n",
       "      <th>Session_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>Height_Acc</th>\n",
       "      <th>Frontness_Acc</th>\n",
       "      <th>Tenseness_Acc</th>\n",
       "      <th>Roundness_Acc</th>\n",
       "      <th>wab1_aq</th>\n",
       "      <th>wab1_nwf_total</th>\n",
       "      <th>Session_Type</th>\n",
       "      <th>Improvement_Group</th>\n",
       "      <th>NWF_Improvement_Group</th>\n",
       "      <th>wabaq_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>B</td>\n",
       "      <td>0.024363</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>No_Improved</td>\n",
       "      <td>NWF_No_Improved</td>\n",
       "      <td>61-70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>UH</td>\n",
       "      <td>0.163408</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>No_Improved</td>\n",
       "      <td>NWF_No_Improved</td>\n",
       "      <td>61-70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>K</td>\n",
       "      <td>0.107875</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>No_Improved</td>\n",
       "      <td>NWF_No_Improved</td>\n",
       "      <td>61-70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>ball</td>\n",
       "      <td>B AO L</td>\n",
       "      <td>0.397365</td>\n",
       "      <td>B</td>\n",
       "      <td>0.014197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>No_Improved</td>\n",
       "      <td>NWF_No_Improved</td>\n",
       "      <td>61-70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>ball</td>\n",
       "      <td>B AO L</td>\n",
       "      <td>0.397365</td>\n",
       "      <td>AO</td>\n",
       "      <td>0.211006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>No_Improved</td>\n",
       "      <td>NWF_No_Improved</td>\n",
       "      <td>61-70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  PID Target Production  Prod_Word_Dur Prod_Arpabet  \\\n",
       "0           0   15   book     B UH K       0.295646            B   \n",
       "1           1   15   book     B UH K       0.295646           UH   \n",
       "2           2   15   book     B UH K       0.295646            K   \n",
       "3           3   15   ball     B AO L       0.397365            B   \n",
       "4           4   15   ball     B AO L       0.397365           AO   \n",
       "\n",
       "   Prod_Phon_Dur                       NOTES  Word_ID  Session_ID  ...  \\\n",
       "0       0.024363  Article (ÃÂ) before word      1.0         0.0  ...   \n",
       "1       0.163408  Article (ÃÂ) before word      1.0         0.0  ...   \n",
       "2       0.107875  Article (ÃÂ) before word      1.0         0.0  ...   \n",
       "3       0.014197                         NaN      2.0         0.0  ...   \n",
       "4       0.211006                         NaN      2.0         0.0  ...   \n",
       "\n",
       "   Height_Acc  Frontness_Acc Tenseness_Acc Roundness_Acc wab1_aq  \\\n",
       "0         1.0            1.0           1.0           1.0    67.8   \n",
       "1         1.0            1.0           1.0           1.0    67.8   \n",
       "2         1.0            1.0           1.0           1.0    67.8   \n",
       "3         1.0            1.0           1.0           1.0    67.8   \n",
       "4         1.0            1.0           1.0           1.0    67.8   \n",
       "\n",
       "   wab1_nwf_total Session_Type Improvement_Group  NWF_Improvement_Group  \\\n",
       "0             3.8     Baseline       No_Improved        NWF_No_Improved   \n",
       "1             3.8     Baseline       No_Improved        NWF_No_Improved   \n",
       "2             3.8     Baseline       No_Improved        NWF_No_Improved   \n",
       "3             3.8     Baseline       No_Improved        NWF_No_Improved   \n",
       "4             3.8     Baseline       No_Improved        NWF_No_Improved   \n",
       "\n",
       "   wabaq_start  \n",
       "0        61-70  \n",
       "1        61-70  \n",
       "2        61-70  \n",
       "3        61-70  \n",
       "4        61-70  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPA_singles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arpabet</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W</th>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CH</th>\n",
       "      <td>ʧ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DH</th>\n",
       "      <td>ð</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DX</th>\n",
       "      <td>ɾ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JH</th>\n",
       "      <td>ʤ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NG</th>\n",
       "      <td>ŋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SH</th>\n",
       "      <td>ʃ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TH</th>\n",
       "      <td>θ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZH</th>\n",
       "      <td>ʒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>ʔ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AA</th>\n",
       "      <td>ɑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AE</th>\n",
       "      <td>æ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AH</th>\n",
       "      <td>ʌ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AO</th>\n",
       "      <td>ɔ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AX</th>\n",
       "      <td>ə</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH</th>\n",
       "      <td>ɛ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EY</th>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IH</th>\n",
       "      <td>ɪ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IY</th>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UH</th>\n",
       "      <td>ʊ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OW</th>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UW</th>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AW</th>\n",
       "      <td>μ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AY</th>\n",
       "      <td>ɐ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AXR</th>\n",
       "      <td>ɚ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ER</th>\n",
       "      <td>ɝ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OY</th>\n",
       "      <td>σ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        IPA_singles\n",
       "Arpabet            \n",
       "H                 h\n",
       "R                 r\n",
       "W                 w\n",
       "Y                 j\n",
       "B                 b\n",
       "CH                ʧ\n",
       "D                 d\n",
       "DH                ð\n",
       "DX                ɾ\n",
       "F                 f\n",
       "G                 g\n",
       "JH                ʤ\n",
       "K                 k\n",
       "L                 l\n",
       "M                 m\n",
       "N                 n\n",
       "NG                ŋ\n",
       "P                 p\n",
       "S                 s\n",
       "SH                ʃ\n",
       "T                 t\n",
       "TH                θ\n",
       "V                 v\n",
       "Z                 z\n",
       "ZH                ʒ\n",
       "Q                 ʔ\n",
       "AA                ɑ\n",
       "AE                æ\n",
       "AH                ʌ\n",
       "AO                ɔ\n",
       "AX                ə\n",
       "EH                ɛ\n",
       "EY                e\n",
       "IH                ɪ\n",
       "IY                i\n",
       "UH                ʊ\n",
       "OW                o\n",
       "UW                u\n",
       "AW                μ\n",
       "AY                ɐ\n",
       "AXR               ɚ\n",
       "ER                ɝ\n",
       "OY                σ"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Store filepath in a variable\n",
    "PVMdat = pd.read_csv(\"Resources/all_data_pvm_acc4.csv\")\n",
    "\n",
    "#Translate the ARPABET codes to IPA codes\n",
    "dictionary = (\n",
    "    pd.read_csv(\"Resources/dict.csv\")\n",
    "    .set_index(\"Arpabet\")\n",
    ")\n",
    "\n",
    "display(PVMdat.head(), dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'PID',\n",
       " 'Target',\n",
       " 'Production',\n",
       " 'Prod_Word_Dur',\n",
       " 'Prod_Arpabet',\n",
       " 'Prod_Phon_Dur',\n",
       " 'NOTES',\n",
       " 'Word_ID',\n",
       " 'Session_ID',\n",
       " 'Prod_Word_N',\n",
       " 'Prod_Phon_N',\n",
       " 'Code',\n",
       " 'Phon_Sess_Code',\n",
       " 'Word_Sess_Code',\n",
       " 'Prod_Last_Phon',\n",
       " 'Prod_Word_IPA',\n",
       " 'Prod_Phon_IPA',\n",
       " 'Prod_Phoneme_ID',\n",
       " 'Prod_Prev_Phon',\n",
       " 'Prod_Next_Phon',\n",
       " 'Target_Arpabet',\n",
       " 'Target_N_Tot_Words',\n",
       " 'Target_Word_IPA',\n",
       " 'Target_N_Tot_Syllables',\n",
       " 'Target_N_Tot_Phonemes',\n",
       " 'Target_Phon_Arpabet',\n",
       " 'Target_Phoneme_ID',\n",
       " 'Target_Syll_Env',\n",
       " 'Target_Word_Pos',\n",
       " 'Syllable_NumID',\n",
       " 'Target_Word_NumID',\n",
       " 'Target_Con_Cluster',\n",
       " 'Target_Clust_ID',\n",
       " 'Target_Clus_Type',\n",
       " 'Target_Clust_Phon_Pos',\n",
       " 'Target_Clust_Phon_Env',\n",
       " 'Target_Phon_IPA',\n",
       " 'Target_Prev_Phon',\n",
       " 'Target_Next_Phon',\n",
       " 'Prod_syllabic',\n",
       " 'Prod_consonantal',\n",
       " 'Prod_sonorant',\n",
       " 'Prod_continuant',\n",
       " 'Prod_delayed release',\n",
       " 'Prod_approximant',\n",
       " 'Prod_tap',\n",
       " 'Prod_nasal',\n",
       " 'Prod_voice',\n",
       " 'Prod_spread gl',\n",
       " 'Prod_constr gl',\n",
       " 'Prod_labial',\n",
       " 'Prod_round',\n",
       " 'Prod_labiodental',\n",
       " 'Prod_coronal',\n",
       " 'Prod_anterior',\n",
       " 'Prod_distributed',\n",
       " 'Prod_strident',\n",
       " 'Prod_lateral',\n",
       " 'Prod_dorsal',\n",
       " 'Prod_high',\n",
       " 'Prod_low',\n",
       " 'Prod_front',\n",
       " 'Prod_back',\n",
       " 'Prod_tense',\n",
       " 'Prod_lax',\n",
       " 'Prod_vowel',\n",
       " 'Prod_consonant',\n",
       " 'Prod_diphthong',\n",
       " 'Prod_monophthong',\n",
       " 'Prod_velar',\n",
       " 'Prod_alveolar',\n",
       " 'Prod_post-alveolar',\n",
       " 'Prod_dental',\n",
       " 'Prod_palatal',\n",
       " 'Prod_glottal',\n",
       " 'Prod_stop',\n",
       " 'Prod_fricative',\n",
       " 'Prod_affricate',\n",
       " 'Prod_glide',\n",
       " 'Prod_Place',\n",
       " 'Prod_Manner',\n",
       " 'Prod_Place_N',\n",
       " 'Prod_Manner_N',\n",
       " 'Prod_Height',\n",
       " 'Prod_Frontness',\n",
       " 'Prod_Height_N',\n",
       " 'Prod_Frontness_N',\n",
       " 'Target_syllabic',\n",
       " 'Target_consonantal',\n",
       " 'Target_sonorant',\n",
       " 'Target_continuant',\n",
       " 'Target_delayed release',\n",
       " 'Target_approximant',\n",
       " 'Target_tap',\n",
       " 'Target_nasal',\n",
       " 'Target_voice',\n",
       " 'Target_spread gl',\n",
       " 'Target_constr gl',\n",
       " 'Target_labial',\n",
       " 'Target_round',\n",
       " 'Target_labiodental',\n",
       " 'Target_coronal',\n",
       " 'Target_anterior',\n",
       " 'Target_distributed',\n",
       " 'Target_strident',\n",
       " 'Target_lateral',\n",
       " 'Target_dorsal',\n",
       " 'Target_high',\n",
       " 'Target_low',\n",
       " 'Target_front',\n",
       " 'Target_back',\n",
       " 'Target_tense',\n",
       " 'Target_lax',\n",
       " 'Target_vowel',\n",
       " 'Target_consonant',\n",
       " 'Target_diphthong',\n",
       " 'Target_monophthong',\n",
       " 'Target_velar',\n",
       " 'Target_alveolar',\n",
       " 'Target_post-alveolar',\n",
       " 'Target_dental',\n",
       " 'Target_palatal',\n",
       " 'Target_glottal',\n",
       " 'Target_stop',\n",
       " 'Target_fricative',\n",
       " 'Target_affricate',\n",
       " 'Target_glide',\n",
       " 'Target_Place',\n",
       " 'Target_Manner',\n",
       " 'Target_Place_N',\n",
       " 'Target_Manner_N',\n",
       " 'Target_Height',\n",
       " 'Target_Frontness',\n",
       " 'Target_Height_N',\n",
       " 'Target_Frontness_N',\n",
       " 'Phon_Acc',\n",
       " 'Voicing_Acc',\n",
       " 'Place_Acc',\n",
       " 'Manner_Acc',\n",
       " 'Height_Acc',\n",
       " 'Frontness_Acc',\n",
       " 'Tenseness_Acc',\n",
       " 'Roundness_Acc',\n",
       " 'wab1_aq',\n",
       " 'wab1_nwf_total',\n",
       " 'Session_Type',\n",
       " 'Improvement_Group',\n",
       " 'NWF_Improvement_Group',\n",
       " 'wabaq_start']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check columns\n",
    "PVMdat.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Production</th>\n",
       "      <th>Prod_Word_Dur</th>\n",
       "      <th>Prod_Arpabet</th>\n",
       "      <th>Prod_Phon_Dur</th>\n",
       "      <th>NOTES</th>\n",
       "      <th>Word_ID</th>\n",
       "      <th>Session_ID</th>\n",
       "      <th>Prod_Word_N</th>\n",
       "      <th>...</th>\n",
       "      <th>Place_Acc</th>\n",
       "      <th>Manner_Acc</th>\n",
       "      <th>Height_Acc</th>\n",
       "      <th>Frontness_Acc</th>\n",
       "      <th>Tenseness_Acc</th>\n",
       "      <th>Roundness_Acc</th>\n",
       "      <th>wab1_aq</th>\n",
       "      <th>wab1_nwf_total</th>\n",
       "      <th>Session_Type</th>\n",
       "      <th>wabaq_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>B</td>\n",
       "      <td>0.024363</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>UH</td>\n",
       "      <td>0.163408</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>K</td>\n",
       "      <td>0.107875</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>ball</td>\n",
       "      <td>B AO L</td>\n",
       "      <td>0.397365</td>\n",
       "      <td>B</td>\n",
       "      <td>0.014197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>ball</td>\n",
       "      <td>B AO L</td>\n",
       "      <td>0.397365</td>\n",
       "      <td>AO</td>\n",
       "      <td>0.211006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PID Target Production  Prod_Word_Dur Prod_Arpabet  Prod_Phon_Dur  \\\n",
       "0   15   book     B UH K       0.295646            B       0.024363   \n",
       "1   15   book     B UH K       0.295646           UH       0.163408   \n",
       "2   15   book     B UH K       0.295646            K       0.107875   \n",
       "3   15   ball     B AO L       0.397365            B       0.014197   \n",
       "4   15   ball     B AO L       0.397365           AO       0.211006   \n",
       "\n",
       "                        NOTES  Word_ID  Session_ID  Prod_Word_N  ...  \\\n",
       "0  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "1  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "2  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "3                         NaN      2.0         0.0          2.0  ...   \n",
       "4                         NaN      2.0         0.0          2.0  ...   \n",
       "\n",
       "   Place_Acc Manner_Acc Height_Acc Frontness_Acc  Tenseness_Acc  \\\n",
       "0        1.0        1.0        1.0           1.0            1.0   \n",
       "1        1.0        1.0        1.0           1.0            1.0   \n",
       "2        1.0        1.0        1.0           1.0            1.0   \n",
       "3        1.0        1.0        1.0           1.0            1.0   \n",
       "4        1.0        1.0        1.0           1.0            1.0   \n",
       "\n",
       "   Roundness_Acc wab1_aq  wab1_nwf_total  Session_Type  wabaq_start  \n",
       "0            1.0    67.8             3.8      Baseline        61-70  \n",
       "1            1.0    67.8             3.8      Baseline        61-70  \n",
       "2            1.0    67.8             3.8      Baseline        61-70  \n",
       "3            1.0    67.8             3.8      Baseline        61-70  \n",
       "4            1.0    67.8             3.8      Baseline        61-70  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The IPA codes we have stored are corrupted, so we need to make them again\n",
    "df = PVMdat[[\n",
    "    'PID',\n",
    "    'Target',\n",
    "    'Production',\n",
    "    'Prod_Word_Dur',\n",
    "    'Prod_Arpabet',\n",
    "    'Prod_Phon_Dur',\n",
    "    'NOTES',\n",
    "    'Word_ID',\n",
    "    'Session_ID',\n",
    "    'Prod_Word_N',\n",
    "    'Prod_Phon_N',\n",
    "    'Code',\n",
    "    'Phon_Sess_Code',\n",
    "    'Word_Sess_Code',\n",
    "    'Prod_Last_Phon',\n",
    "    'Prod_Phoneme_ID',\n",
    "    'Target_Arpabet',\n",
    "    'Target_N_Tot_Words',\n",
    "    'Target_N_Tot_Syllables',\n",
    "    'Target_N_Tot_Phonemes',\n",
    "    'Target_Phon_Arpabet',\n",
    "    'Target_Phoneme_ID',\n",
    "    'Target_Syll_Env',\n",
    "    'Target_Word_Pos',\n",
    "    'Syllable_NumID',\n",
    "    'Target_Word_NumID',\n",
    "    'Target_Con_Cluster',\n",
    "    'Target_Clust_ID',\n",
    "    'Target_Clus_Type',\n",
    "    'Target_Clust_Phon_Pos',\n",
    "    'Target_Clust_Phon_Env',\n",
    "    'Target_Prev_Phon',\n",
    "    'Target_Next_Phon',\n",
    "    'Prod_syllabic',\n",
    "    'Prod_consonantal',\n",
    "    'Prod_sonorant',\n",
    "    'Prod_continuant',\n",
    "    'Prod_delayed release',\n",
    "    'Prod_approximant',\n",
    "    'Prod_tap',\n",
    "    'Prod_nasal',\n",
    "    'Prod_voice',\n",
    "    'Prod_spread gl',\n",
    "    'Prod_constr gl',\n",
    "    'Prod_labial',\n",
    "    'Prod_round',\n",
    "    'Prod_labiodental',\n",
    "    'Prod_coronal',\n",
    "    'Prod_anterior',\n",
    "    'Prod_distributed',\n",
    "    'Prod_strident',\n",
    "    'Prod_lateral',\n",
    "    'Prod_dorsal',\n",
    "    'Prod_high',\n",
    "    'Prod_low',\n",
    "    'Prod_front',\n",
    "    'Prod_back',\n",
    "    'Prod_tense',\n",
    "    'Prod_lax',\n",
    "    'Prod_vowel',\n",
    "    'Prod_consonant',\n",
    "    'Prod_diphthong',\n",
    "    'Prod_monophthong',\n",
    "    'Prod_velar',\n",
    "    'Prod_alveolar',\n",
    "    'Prod_post-alveolar',\n",
    "    'Prod_dental',\n",
    "    'Prod_palatal',\n",
    "    'Prod_glottal',\n",
    "    'Prod_stop',\n",
    "    'Prod_fricative',\n",
    "    'Prod_affricate',\n",
    "    'Prod_glide',\n",
    "    'Prod_Place',\n",
    "    'Prod_Manner',\n",
    "    'Prod_Height',\n",
    "    'Prod_Frontness',\n",
    "    'Target_syllabic',\n",
    "    'Target_consonantal',\n",
    "    'Target_sonorant',\n",
    "    'Target_continuant',\n",
    "    'Target_delayed release',\n",
    "    'Target_approximant',\n",
    "    'Target_tap',\n",
    "    'Target_nasal',\n",
    "    'Target_voice',\n",
    "    'Target_spread gl',\n",
    "    'Target_constr gl',\n",
    "    'Target_labial',\n",
    "    'Target_round',\n",
    "    'Target_labiodental',\n",
    "    'Target_coronal',\n",
    "    'Target_anterior',\n",
    "    'Target_distributed',\n",
    "    'Target_strident',\n",
    "    'Target_lateral',\n",
    "    'Target_dorsal',\n",
    "    'Target_high',\n",
    "    'Target_low',\n",
    "    'Target_front',\n",
    "    'Target_back',\n",
    "    'Target_tense',\n",
    "    'Target_lax',\n",
    "    'Target_vowel',\n",
    "    'Target_consonant',\n",
    "    'Target_diphthong',\n",
    "    'Target_monophthong',\n",
    "    'Target_velar',\n",
    "    'Target_alveolar',\n",
    "    'Target_post-alveolar',\n",
    "    'Target_dental',\n",
    "    'Target_palatal',\n",
    "    'Target_glottal',\n",
    "    'Target_stop',\n",
    "    'Target_fricative',\n",
    "    'Target_affricate',\n",
    "    'Target_glide',\n",
    "    'Target_Place',\n",
    "    'Target_Manner',\n",
    "    'Target_Height',\n",
    "    'Target_Frontness',\n",
    "    'Phon_Acc',\n",
    "    'Voicing_Acc',\n",
    "    'Place_Acc',\n",
    "    'Manner_Acc',\n",
    "    'Height_Acc',\n",
    "    'Frontness_Acc',\n",
    "    'Tenseness_Acc',\n",
    "    'Roundness_Acc',\n",
    "    'wab1_aq',\n",
    "    'wab1_nwf_total',\n",
    "    'Session_Type',\n",
    "    'wabaq_start'\n",
    "]]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B UH K', 'B AO L', 'N AY F', 'K AH P', 'S EY F T IY P IH N',\n",
       "       'H AE M AXR', 'T UW TH B R AX SH', 'AX R EY S AXR ', 'L AA K',\n",
       "       'P EH N S AX L', 'S K R UW D R AY V AXR', 'K IY',\n",
       "       'P EY P AXR K L IH P', 'W AA CH', 'K OW M', 'R AH B AXR B AE N D',\n",
       "       'S P UW N', 'T EY P', 'F AO R K', 'M AE CH AX Z'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check that target words are correctly written\n",
    "df['Target_Arpabet'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/y7yy3gjx1vjcj8dmj6zw15kw0000gn/T/ipykernel_21173/1572579096.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Target_Arpabet'] = (\n"
     ]
    }
   ],
   "source": [
    "#Replace incorrect target words\n",
    "df['Target_Arpabet'] = (\n",
    "    df['Target_Arpabet']\n",
    "    .replace(\n",
    "        {\n",
    "        'AX R EY S AXR ':'AX R EY S AXR'\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B', 'UH', 'K', 'AO', 'L', 'N', 'AY', 'F', 'AH', 'P', 'S', 'EY',\n",
       "       'T', 'IY', 'IH', 'H', 'AE', 'M', 'AXR', 'UW', 'TH', 'R', 'AX',\n",
       "       'SH', 'AA', 'EH', 'D', 'V', 'W', 'CH', 'OW', 'Z'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check what each phoneme is being registered as\n",
    "results = (\n",
    "    # trans is a series, so use string accessor to split value strings\n",
    "    df[\"Target_Arpabet\"].str.split(\" \")\n",
    "    # turn each item in split string into own row maintaining index value\n",
    "    .explode()\n",
    ")\n",
    "\n",
    "results.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B UH K', 'B AO L', 'SH', 'N', 'N AY F', 'K AH P', 'S',\n",
       "       'S AY F T IY', 'H AE M AXR', 'T UW SH B OW N', 'T UW', 'B UH SH',\n",
       "       'T UH', 'T IY', 'T UW TH P IY S', 'T P IY', 'T IY S', 'P IY', 'P',\n",
       "       'IY', 'R AH B', 'R AH B AXR N', 'R AH B AXR', 'L AA K AXR',\n",
       "       'T EH M', 'T', 'P EH N S AX L', 'S T R UW', 'S T R',\n",
       "       'S T R UW DX OW', 'S K R UW', 'S K R UW CH', 'S K R UW S', 'K',\n",
       "       'ZH', 'K IY', 'T S', 'S IH M', 'P EY P AXR', 'W AA CH', 'OW',\n",
       "       'R AH B AXR L', 'R AH B AXR N AX Q', 'R AH', 'K AH', 'Q', 'S P UW',\n",
       "       'S P UH SH', 'S P', 'S B AH', 'S UW P', 'T EY', 'SH AO R', 'B AX',\n",
       "       'AY F', 'AY', 'K AO', 'K AH F', 'K AO Q', 'K AO F', 'K AH Q',\n",
       "       'S T EY Q', 'S T EY', 'S T AY', 'H', 'H AE', 'T IH TH', 'P EY S',\n",
       "       'B AA', 'P AH', 'B', 'B AH', 'L', 'L AA K', 'P EH N CH R',\n",
       "       'P EH N S IH', 'P EH N', 'P EH', 'P IH N', 'K OW', 'K AX',\n",
       "       'S T IH F', 'S P UW N', 'T AE', 'P AX', 'P IH', 'P T S', 'F AX',\n",
       "       'F', 'S EY F T IY', 'S AH', 'S S', 'R', 'T UW S', 'T UW TH',\n",
       "       'P EH N T', 'P EH N T AX L', 'P EH S', 'W AO', 'W AA', 'W AA Q',\n",
       "       'R OW', 'L EH Q', 'K AO F IY', 'S EY F S EY F T IY', 'S EY F',\n",
       "       'S S EY F T IY', 'S IY', 'K OW N', 'H AE T', 'B R AH SH',\n",
       "       'T UW TH B R AH SH', 'EH S', 'B IY', 'B AH S', 'B IY S',\n",
       "       'R EY S AXR', 'L AH K', 'JH EY', 'JH IY', 'S K R UW B AX',\n",
       "       'S K R UW B AY', 'S K R UW B IH T', 'S K R UW B', 'S IY OW',\n",
       "       'S IY OW EH N', 'H EH R', 'K OW T', 'K OW M', 'R OW P',\n",
       "       'R AH B AXR B AE N D', 'T R IY', 'K AX S AH M TH IH NG',\n",
       "       'S P UW P', 'S K UW P', 'B IY CH', 'T EY P', 'F UH T', 'F AO R K',\n",
       "       'S IH G AX R EH T', 'M EH T', 'M AE Q', 'M AE', 'M AE SH',\n",
       "       'M AE CH IH Z', 'H IY H AE', 'K AH R', 'D IY', 'EY', 'D R AY',\n",
       "       'D AO G', 'D AO', 'D IY OW Q AH R', 'T AX', 'H AE N D AX L',\n",
       "       'F AO R W EY', 'F AO R', 'D EY', 'B IH T', 'N EY L Z', 'N IY L Z',\n",
       "       'H AE M', 'T UW TH K IH T', 'S EY', 'T R UW TH', 'CH', 'B UW T S',\n",
       "       'T UW TH S', 'S IH T', 'S EH', 'IY R', 'R EY S', 'M AE S T AXR',\n",
       "       'S K R UW D R AY V AXR', 'P EY P AXR K L IH P', 'W AH CH',\n",
       "       'K OW Q', 'K AA Q', 'R AH B AX R', 'T EY K AX N', 'T EY K',\n",
       "       'M AE S', 'S M EH CH IH Z', 'S EY F T IY G AY K', 'S EH T',\n",
       "       'S EY F T IY P IH N', 'H AE B', 'H AE V', 'IH R EY S AXR',\n",
       "       'T UW TH W IH Z', 'T UW TH IH S', 'S UW', 'S K R UW JH AXR',\n",
       "       'S K UW T AXR', 'K IY Z', 'K AO L', 'K OW L', 'T AE K', 'K EY',\n",
       "       'H AE M AX R', 'T UW TH B R AX SH', 'AX R EY S AXR',\n",
       "       'M AE CH AX Z', 'K IY S', 'S UW S', 'IH', 'B AA L', 'K UH P',\n",
       "       'B R UH CH AXR T IY TH', 'B R UH SH T IY TH', 'B R UH SH',\n",
       "       'EH R EY S AXR', 'P EH N S L', 'S R UW D R OW V AXR D R AY V AXR',\n",
       "       'R UH B AXR B AE N D', 'T AE T', 'B OW T', 'T UW TH B EH SH',\n",
       "       'T UW TH B R UH SH', 'L EY D AXR', 'S UW N', 'M AE CH IH S',\n",
       "       'H AA R', 'D R AY V AXR', 'P EY P AXR K L IH', 'K K UH M',\n",
       "       'F AY AXR', 'F R AA G', 'AA R Y UW', 'K UH', 'S K UW D R AY V AXR',\n",
       "       'P EY P AXR D EY', 'W AA SH', 'R UH B AXR R UH B AXR B AE N D',\n",
       "       'F OW R K', 'M AE CH', 'M AE CH B UH K', 'D AA G',\n",
       "       'G UH D AA K AA R L', 'UH R EY S AXR', 'M UH K AE N IH K',\n",
       "       'P IH F', 'P EY P AXR K IH', 'K L IH P', 'K UH M',\n",
       "       'R UH B AXR B AE N T', 'M', 'B UH K B R UH K', 'B R UH K', 'N IH',\n",
       "       'M AE N AXR', 'AE M AXR', 'T UW TH B R UH K', 'EH L', 'L EY AXR',\n",
       "       'JH R AY V AXR', 'K AA M K AA M', 'R UH B AXR', 'S K AA CH',\n",
       "       'S K AA CH T EY', 'M AE UH CH', 'M AE CH IH', 'M AE CH IH T IY',\n",
       "       'M AE CH Y AE', 'R IY M OW T', 'IY R EY S', 'T AY S T IY K OW M',\n",
       "       'S AY F T IY P IH N', 'IY R EY S AXR', 'G EH DX IH N',\n",
       "       'K R UW D R AY V AXR', 'B AXR B AE N D', 'UW D R AY V AXR',\n",
       "       'AA P AH N', 'AXR H AE M AXR', 'T UW TH P EY S', 'W AO CH',\n",
       "       'T UW Q B R AH SH', 'P EY S T', 'T UW TH P EY S T', 'L AO K',\n",
       "       'S K', 'S K R UW B UW T', 'S K R UW D IY R', 'S K R UW D IY AX',\n",
       "       'R AX B AXR B AE N D', nan, 'M AE CH B AA K S', 'B AA B IY P IH N',\n",
       "       'S S K R UW D R AY V AXR', 'R AX B AXR B AE N', 'R AX B AXR',\n",
       "       'P EY S T AXR', 'CH AA R JH', 'S EY F T IY K L IH P',\n",
       "       'S EY F T IY P EH N'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check that productions are correctly written\n",
    "df['Production'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Production'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PID',\n",
       " 'Target',\n",
       " 'Production',\n",
       " 'Prod_Word_Dur',\n",
       " 'Prod_Arpabet',\n",
       " 'Prod_Phon_Dur',\n",
       " 'NOTES',\n",
       " 'Word_ID',\n",
       " 'Session_ID',\n",
       " 'Prod_Word_N',\n",
       " 'Prod_Phon_N',\n",
       " 'Code',\n",
       " 'Phon_Sess_Code',\n",
       " 'Word_Sess_Code',\n",
       " 'Prod_Last_Phon',\n",
       " 'Prod_Phoneme_ID',\n",
       " 'Target_Arpabet',\n",
       " 'Target_N_Tot_Words',\n",
       " 'Target_N_Tot_Syllables',\n",
       " 'Target_N_Tot_Phonemes',\n",
       " 'Target_Phon_Arpabet',\n",
       " 'Target_Phoneme_ID',\n",
       " 'Target_Syll_Env',\n",
       " 'Target_Word_Pos',\n",
       " 'Syllable_NumID',\n",
       " 'Target_Word_NumID',\n",
       " 'Target_Con_Cluster',\n",
       " 'Target_Clust_ID',\n",
       " 'Target_Clus_Type',\n",
       " 'Target_Clust_Phon_Pos',\n",
       " 'Target_Clust_Phon_Env',\n",
       " 'Target_Prev_Phon',\n",
       " 'Target_Next_Phon',\n",
       " 'Prod_syllabic',\n",
       " 'Prod_consonantal',\n",
       " 'Prod_sonorant',\n",
       " 'Prod_continuant',\n",
       " 'Prod_delayed release',\n",
       " 'Prod_approximant',\n",
       " 'Prod_tap',\n",
       " 'Prod_nasal',\n",
       " 'Prod_voice',\n",
       " 'Prod_spread gl',\n",
       " 'Prod_constr gl',\n",
       " 'Prod_labial',\n",
       " 'Prod_round',\n",
       " 'Prod_labiodental',\n",
       " 'Prod_coronal',\n",
       " 'Prod_anterior',\n",
       " 'Prod_distributed',\n",
       " 'Prod_strident',\n",
       " 'Prod_lateral',\n",
       " 'Prod_dorsal',\n",
       " 'Prod_high',\n",
       " 'Prod_low',\n",
       " 'Prod_front',\n",
       " 'Prod_back',\n",
       " 'Prod_tense',\n",
       " 'Prod_lax',\n",
       " 'Prod_vowel',\n",
       " 'Prod_consonant',\n",
       " 'Prod_diphthong',\n",
       " 'Prod_monophthong',\n",
       " 'Prod_velar',\n",
       " 'Prod_alveolar',\n",
       " 'Prod_post-alveolar',\n",
       " 'Prod_dental',\n",
       " 'Prod_palatal',\n",
       " 'Prod_glottal',\n",
       " 'Prod_stop',\n",
       " 'Prod_fricative',\n",
       " 'Prod_affricate',\n",
       " 'Prod_glide',\n",
       " 'Prod_Place',\n",
       " 'Prod_Manner',\n",
       " 'Prod_Height',\n",
       " 'Prod_Frontness',\n",
       " 'Target_syllabic',\n",
       " 'Target_consonantal',\n",
       " 'Target_sonorant',\n",
       " 'Target_continuant',\n",
       " 'Target_delayed release',\n",
       " 'Target_approximant',\n",
       " 'Target_tap',\n",
       " 'Target_nasal',\n",
       " 'Target_voice',\n",
       " 'Target_spread gl',\n",
       " 'Target_constr gl',\n",
       " 'Target_labial',\n",
       " 'Target_round',\n",
       " 'Target_labiodental',\n",
       " 'Target_coronal',\n",
       " 'Target_anterior',\n",
       " 'Target_distributed',\n",
       " 'Target_strident',\n",
       " 'Target_lateral',\n",
       " 'Target_dorsal',\n",
       " 'Target_high',\n",
       " 'Target_low',\n",
       " 'Target_front',\n",
       " 'Target_back',\n",
       " 'Target_tense',\n",
       " 'Target_lax',\n",
       " 'Target_vowel',\n",
       " 'Target_consonant',\n",
       " 'Target_diphthong',\n",
       " 'Target_monophthong',\n",
       " 'Target_velar',\n",
       " 'Target_alveolar',\n",
       " 'Target_post-alveolar',\n",
       " 'Target_dental',\n",
       " 'Target_palatal',\n",
       " 'Target_glottal',\n",
       " 'Target_stop',\n",
       " 'Target_fricative',\n",
       " 'Target_affricate',\n",
       " 'Target_glide',\n",
       " 'Target_Place',\n",
       " 'Target_Manner',\n",
       " 'Target_Height',\n",
       " 'Target_Frontness',\n",
       " 'Phon_Acc',\n",
       " 'Voicing_Acc',\n",
       " 'Place_Acc',\n",
       " 'Manner_Acc',\n",
       " 'Height_Acc',\n",
       " 'Frontness_Acc',\n",
       " 'Tenseness_Acc',\n",
       " 'Roundness_Acc',\n",
       " 'wab1_aq',\n",
       " 'wab1_nwf_total',\n",
       " 'Session_Type',\n",
       " 'wabaq_start',\n",
       " 'Target_Word_IPA']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Target_Word_IPA\"] = (\n",
    "    # The production column is a series, so use string accessor to split value strings\n",
    "    df[\"Target_Arpabet\"].str.split(\" \")\n",
    "    # turn each item in split string into own row maintaining index value\n",
    "    .explode()\n",
    "    # perform the lookup in the dictionary of each individual value\n",
    "    .apply(lambda v: dictionary.loc[v])\n",
    "    # group them by the original index\n",
    "    .groupby(level=0)\n",
    "    # \"sum\" them, which for string, concatonates them without any spaces\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "df.columns.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Production</th>\n",
       "      <th>Prod_Word_Dur</th>\n",
       "      <th>Prod_Arpabet</th>\n",
       "      <th>Prod_Phon_Dur</th>\n",
       "      <th>NOTES</th>\n",
       "      <th>Word_ID</th>\n",
       "      <th>Session_ID</th>\n",
       "      <th>Prod_Word_N</th>\n",
       "      <th>...</th>\n",
       "      <th>Height_Acc</th>\n",
       "      <th>Frontness_Acc</th>\n",
       "      <th>Tenseness_Acc</th>\n",
       "      <th>Roundness_Acc</th>\n",
       "      <th>wab1_aq</th>\n",
       "      <th>wab1_nwf_total</th>\n",
       "      <th>Session_Type</th>\n",
       "      <th>wabaq_start</th>\n",
       "      <th>Target_Word_IPA</th>\n",
       "      <th>Prod_Word_IPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>B</td>\n",
       "      <td>0.024363</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>bʊk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>UH</td>\n",
       "      <td>0.163408</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>bʊk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>K</td>\n",
       "      <td>0.107875</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>bʊk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>ball</td>\n",
       "      <td>B AO L</td>\n",
       "      <td>0.397365</td>\n",
       "      <td>B</td>\n",
       "      <td>0.014197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "      <td>bɔl</td>\n",
       "      <td>bɔl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>ball</td>\n",
       "      <td>B AO L</td>\n",
       "      <td>0.397365</td>\n",
       "      <td>AO</td>\n",
       "      <td>0.211006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "      <td>bɔl</td>\n",
       "      <td>bɔl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PID Target Production  Prod_Word_Dur Prod_Arpabet  Prod_Phon_Dur  \\\n",
       "0   15   book     B UH K       0.295646            B       0.024363   \n",
       "1   15   book     B UH K       0.295646           UH       0.163408   \n",
       "2   15   book     B UH K       0.295646            K       0.107875   \n",
       "3   15   ball     B AO L       0.397365            B       0.014197   \n",
       "4   15   ball     B AO L       0.397365           AO       0.211006   \n",
       "\n",
       "                        NOTES  Word_ID  Session_ID  Prod_Word_N  ...  \\\n",
       "0  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "1  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "2  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "3                         NaN      2.0         0.0          2.0  ...   \n",
       "4                         NaN      2.0         0.0          2.0  ...   \n",
       "\n",
       "   Height_Acc Frontness_Acc Tenseness_Acc Roundness_Acc  wab1_aq  \\\n",
       "0         1.0           1.0           1.0           1.0     67.8   \n",
       "1         1.0           1.0           1.0           1.0     67.8   \n",
       "2         1.0           1.0           1.0           1.0     67.8   \n",
       "3         1.0           1.0           1.0           1.0     67.8   \n",
       "4         1.0           1.0           1.0           1.0     67.8   \n",
       "\n",
       "   wab1_nwf_total Session_Type  wabaq_start  Target_Word_IPA  Prod_Word_IPA  \n",
       "0             3.8     Baseline        61-70              bʊk            bʊk  \n",
       "1             3.8     Baseline        61-70              bʊk            bʊk  \n",
       "2             3.8     Baseline        61-70              bʊk            bʊk  \n",
       "3             3.8     Baseline        61-70              bɔl            bɔl  \n",
       "4             3.8     Baseline        61-70              bɔl            bɔl  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Prod_Word_IPA\"] = (\n",
    "    # The production column is a series, so use string accessor to split value strings\n",
    "    df[\"Production\"].str.split(\" \")\n",
    "    # turn each item in split string into own row maintaining index value\n",
    "    .explode()\n",
    "    # perform the lookup in the dictionary of each individual value\n",
    "    .apply(lambda v: dictionary.loc[v])\n",
    "    # group them by the original index\n",
    "    .groupby(level=0)\n",
    "    # \"sum\" them, which for string, concatonates them without any spaces\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Get number of phonemes for each production\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mProd_N_Tot_Phonemes\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[0;32m----> 3\u001b[0m     df\n\u001b[1;32m      4\u001b[0m     \u001b[39m.\u001b[39mapply(\n\u001b[1;32m      5\u001b[0m         \u001b[39mlambda\u001b[39;00m x:\n\u001b[1;32m      6\u001b[0m         \u001b[39mlen\u001b[39m(x[\u001b[39m'\u001b[39m\u001b[39mProd_Word_IPA\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m      7\u001b[0m         axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[39m#Get number of phonemes for each target\u001b[39;00m\n\u001b[1;32m     12\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mTarget_N_Tot_Phonemes\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[1;32m     13\u001b[0m     df\n\u001b[1;32m     14\u001b[0m     \u001b[39m.\u001b[39mapply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Get number of phonemes for each production\n",
    "df['Prod_N_Tot_Phonemes'] = (\n",
    "    df\n",
    "    .apply(\n",
    "        lambda x:\n",
    "        len(x['Prod_Word_IPA']),\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "#Get number of phonemes for each target\n",
    "df['Target_N_Tot_Phonemes'] = (\n",
    "    df\n",
    "    .apply(\n",
    "        lambda x:\n",
    "        len(x['Target_Word_IPA']),\n",
    "        axis=1\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace incorrectly coded words\n",
    "df['Prod_Word_IPA'] = (\n",
    "    df['Prod_Word_IPA']\n",
    "    .replace(\n",
    "        {\n",
    "        'i':'ə', \n",
    "        'ir':'ər', \n",
    "        'ɪ':'ə', \n",
    "        'ɛresɚ':'əresɚ',\n",
    "        'ires':'əresɚ',\n",
    "        'ɪresɚ':'əresɚ', \n",
    "        'iresɚ':'əresɚ'\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import phonetic feature identifies\n",
    "phon_dist_features = (\n",
    "    pd.read_csv(\"Resources/phon_dist_features.csv\")\n",
    ")\n",
    "phon_dist_features = phon_dist_features.dropna()\n",
    "phon_dist_features['Phoneme_ID'] = phon_dist_features['Phoneme_ID'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target_Phoneme_IPA</th>\n",
       "      <th>Target_Phoneme_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>j</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ʧ</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>d</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ð</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ɾ</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>f</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>g</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ʤ</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>k</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>l</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>m</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>n</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ŋ</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>p</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>s</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ʃ</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>t</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>θ</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>v</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>z</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ʒ</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ʔ</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ɑ</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>æ</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ʌ</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ɔ</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ə</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ɛ</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>e</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ɪ</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>i</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ʊ</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>o</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>u</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>μ</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ɐ</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ɚ</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ɝ</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>σ</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>*</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target_Phoneme_IPA  Target_Phoneme_ID\n",
       "0                   h                  1\n",
       "1                   r                  2\n",
       "2                   w                  3\n",
       "3                   j                  4\n",
       "4                   b                  5\n",
       "5                   ʧ                  6\n",
       "6                   d                  7\n",
       "7                   ð                  8\n",
       "8                   ɾ                  9\n",
       "9                   f                 10\n",
       "10                  g                 11\n",
       "11                  ʤ                 12\n",
       "12                  k                 13\n",
       "13                  l                 14\n",
       "14                  m                 15\n",
       "15                  n                 16\n",
       "16                  ŋ                 17\n",
       "17                  p                 18\n",
       "18                  s                 19\n",
       "19                  ʃ                 20\n",
       "20                  t                 21\n",
       "21                  θ                 22\n",
       "22                  v                 23\n",
       "23                  z                 24\n",
       "24                  ʒ                 25\n",
       "25                  ʔ                 26\n",
       "26                  ɑ                 27\n",
       "27                  æ                 28\n",
       "28                  ʌ                 29\n",
       "29                  ɔ                 30\n",
       "30                  ə                 31\n",
       "31                  ɛ                 32\n",
       "32                  e                 33\n",
       "33                  ɪ                 34\n",
       "34                  i                 35\n",
       "35                  ʊ                 36\n",
       "36                  o                 37\n",
       "37                  u                 38\n",
       "38                  μ                 39\n",
       "39                  ɐ                 40\n",
       "40                  ɚ                 41\n",
       "41                  ɝ                 42\n",
       "42                  σ                 43\n",
       "43                  *                 44"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dictionary for phoneme ID number\n",
    "\n",
    "Prod_phon_ID = phon_dist_features[['IPA_singles','Phoneme_ID']].copy()\n",
    "Target_phon_ID = phon_dist_features[['IPA_singles','Phoneme_ID']].copy()\n",
    "\n",
    "\n",
    "# Creat dictionary for Prod_Phoneme_ID\n",
    "Prod_phon_ID.rename(\n",
    "    columns={\n",
    "       'IPA_singles':'Prod_Phoneme_IPA', \n",
    "       'Phoneme_ID':'Prod_Phoneme_ID'\n",
    "       }, inplace=True)\n",
    "\n",
    "Target_phon_ID.rename(\n",
    "    columns={\n",
    "       'IPA_singles':'Target_Phoneme_IPA', \n",
    "       'Phoneme_ID':'Target_Phoneme_ID'\n",
    "       }, inplace=True)\n",
    "\n",
    "Prod_phon_ID\n",
    "Target_phon_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Production</th>\n",
       "      <th>Prod_Word_Dur</th>\n",
       "      <th>Prod_Arpabet</th>\n",
       "      <th>Prod_Phon_Dur</th>\n",
       "      <th>NOTES</th>\n",
       "      <th>Word_ID</th>\n",
       "      <th>Session_ID</th>\n",
       "      <th>Prod_Word_N</th>\n",
       "      <th>...</th>\n",
       "      <th>Roundness_Acc</th>\n",
       "      <th>wab1_aq</th>\n",
       "      <th>wab1_nwf_total</th>\n",
       "      <th>Session_Type</th>\n",
       "      <th>wabaq_start</th>\n",
       "      <th>Target_Word_IPA</th>\n",
       "      <th>Prod_Word_IPA</th>\n",
       "      <th>Prod_N_Tot_Phonemes</th>\n",
       "      <th>Prod_Phoneme_IPA</th>\n",
       "      <th>Target_Phoneme_IPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>B</td>\n",
       "      <td>0.024363</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>3</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>UH</td>\n",
       "      <td>0.163408</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>3</td>\n",
       "      <td>ʊ</td>\n",
       "      <td>ʊ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>K</td>\n",
       "      <td>0.107875</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>3</td>\n",
       "      <td>k</td>\n",
       "      <td>k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>ball</td>\n",
       "      <td>B AO L</td>\n",
       "      <td>0.397365</td>\n",
       "      <td>B</td>\n",
       "      <td>0.014197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "      <td>bɔl</td>\n",
       "      <td>bɔl</td>\n",
       "      <td>3</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>ball</td>\n",
       "      <td>B AO L</td>\n",
       "      <td>0.397365</td>\n",
       "      <td>AO</td>\n",
       "      <td>0.211006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>61-70</td>\n",
       "      <td>bɔl</td>\n",
       "      <td>bɔl</td>\n",
       "      <td>3</td>\n",
       "      <td>ɔ</td>\n",
       "      <td>ɔ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PID Target Production  Prod_Word_Dur Prod_Arpabet  Prod_Phon_Dur  \\\n",
       "0   15   book     B UH K       0.295646            B       0.024363   \n",
       "1   15   book     B UH K       0.295646           UH       0.163408   \n",
       "2   15   book     B UH K       0.295646            K       0.107875   \n",
       "3   15   ball     B AO L       0.397365            B       0.014197   \n",
       "4   15   ball     B AO L       0.397365           AO       0.211006   \n",
       "\n",
       "                        NOTES  Word_ID  Session_ID  Prod_Word_N  ...  \\\n",
       "0  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "1  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "2  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "3                         NaN      2.0         0.0          2.0  ...   \n",
       "4                         NaN      2.0         0.0          2.0  ...   \n",
       "\n",
       "   Roundness_Acc wab1_aq wab1_nwf_total Session_Type  wabaq_start  \\\n",
       "0            1.0    67.8            3.8     Baseline        61-70   \n",
       "1            1.0    67.8            3.8     Baseline        61-70   \n",
       "2            1.0    67.8            3.8     Baseline        61-70   \n",
       "3            1.0    67.8            3.8     Baseline        61-70   \n",
       "4            1.0    67.8            3.8     Baseline        61-70   \n",
       "\n",
       "   Target_Word_IPA Prod_Word_IPA  Prod_N_Tot_Phonemes  Prod_Phoneme_IPA  \\\n",
       "0              bʊk           bʊk                    3                 b   \n",
       "1              bʊk           bʊk                    3                 ʊ   \n",
       "2              bʊk           bʊk                    3                 k   \n",
       "3              bɔl           bɔl                    3                 b   \n",
       "4              bɔl           bɔl                    3                 ɔ   \n",
       "\n",
       "   Target_Phoneme_IPA  \n",
       "0                   b  \n",
       "1                   ʊ  \n",
       "2                   k  \n",
       "3                   b  \n",
       "4                   ɔ  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge with original dataset\n",
    "df = df.merge(Prod_phon_ID, on='Prod_Phoneme_ID', how='left').merge(Target_phon_ID, on='Target_Phoneme_ID', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Damerau_Levenshtein \n",
    "df['Damerau_Levenshtein'] = (\n",
    "    df\n",
    "    .apply(\n",
    "        lambda x:\n",
    "        td.damerau_levenshtein(str(x['Target_Word_IPA']),str(x['Prod_Word_IPA'])),\n",
    "        axis=1\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.110030395136778"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Damerau_Levenshtein'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PID',\n",
       " 'Target',\n",
       " 'Production',\n",
       " 'Prod_Word_Dur',\n",
       " 'Prod_Arpabet',\n",
       " 'Prod_Phon_Dur',\n",
       " 'NOTES',\n",
       " 'Word_ID',\n",
       " 'Session_ID',\n",
       " 'Prod_Word_N',\n",
       " 'Prod_Phon_N',\n",
       " 'Code',\n",
       " 'Phon_Sess_Code',\n",
       " 'Word_Sess_Code',\n",
       " 'Prod_Last_Phon',\n",
       " 'Prod_Phoneme_ID',\n",
       " 'Target_Arpabet',\n",
       " 'Target_N_Tot_Words',\n",
       " 'Target_N_Tot_Syllables',\n",
       " 'Target_N_Tot_Phonemes',\n",
       " 'Target_Phon_Arpabet',\n",
       " 'Target_Phoneme_ID',\n",
       " 'Target_Syll_Env',\n",
       " 'Target_Word_Pos',\n",
       " 'Syllable_NumID',\n",
       " 'Target_Word_NumID',\n",
       " 'Target_Con_Cluster',\n",
       " 'Target_Clust_ID',\n",
       " 'Target_Clus_Type',\n",
       " 'Target_Clust_Phon_Pos',\n",
       " 'Target_Clust_Phon_Env',\n",
       " 'Target_Prev_Phon',\n",
       " 'Target_Next_Phon',\n",
       " 'Prod_syllabic',\n",
       " 'Prod_consonantal',\n",
       " 'Prod_sonorant',\n",
       " 'Prod_continuant',\n",
       " 'Prod_delayed release',\n",
       " 'Prod_approximant',\n",
       " 'Prod_tap',\n",
       " 'Prod_nasal',\n",
       " 'Prod_voice',\n",
       " 'Prod_spread gl',\n",
       " 'Prod_constr gl',\n",
       " 'Prod_labial',\n",
       " 'Prod_round',\n",
       " 'Prod_labiodental',\n",
       " 'Prod_coronal',\n",
       " 'Prod_anterior',\n",
       " 'Prod_distributed',\n",
       " 'Prod_strident',\n",
       " 'Prod_lateral',\n",
       " 'Prod_dorsal',\n",
       " 'Prod_high',\n",
       " 'Prod_low',\n",
       " 'Prod_front',\n",
       " 'Prod_back',\n",
       " 'Prod_tense',\n",
       " 'Prod_lax',\n",
       " 'Prod_vowel',\n",
       " 'Prod_consonant',\n",
       " 'Prod_diphthong',\n",
       " 'Prod_monophthong',\n",
       " 'Prod_velar',\n",
       " 'Prod_alveolar',\n",
       " 'Prod_post-alveolar',\n",
       " 'Prod_dental',\n",
       " 'Prod_palatal',\n",
       " 'Prod_glottal',\n",
       " 'Prod_stop',\n",
       " 'Prod_fricative',\n",
       " 'Prod_affricate',\n",
       " 'Prod_glide',\n",
       " 'Prod_Place',\n",
       " 'Prod_Manner',\n",
       " 'Prod_Height',\n",
       " 'Prod_Frontness',\n",
       " 'Target_syllabic',\n",
       " 'Target_consonantal',\n",
       " 'Target_sonorant',\n",
       " 'Target_continuant',\n",
       " 'Target_delayed release',\n",
       " 'Target_approximant',\n",
       " 'Target_tap',\n",
       " 'Target_nasal',\n",
       " 'Target_voice',\n",
       " 'Target_spread gl',\n",
       " 'Target_constr gl',\n",
       " 'Target_labial',\n",
       " 'Target_round',\n",
       " 'Target_labiodental',\n",
       " 'Target_coronal',\n",
       " 'Target_anterior',\n",
       " 'Target_distributed',\n",
       " 'Target_strident',\n",
       " 'Target_lateral',\n",
       " 'Target_dorsal',\n",
       " 'Target_high',\n",
       " 'Target_low',\n",
       " 'Target_front',\n",
       " 'Target_back',\n",
       " 'Target_tense',\n",
       " 'Target_lax',\n",
       " 'Target_vowel',\n",
       " 'Target_consonant',\n",
       " 'Target_diphthong',\n",
       " 'Target_monophthong',\n",
       " 'Target_velar',\n",
       " 'Target_alveolar',\n",
       " 'Target_post-alveolar',\n",
       " 'Target_dental',\n",
       " 'Target_palatal',\n",
       " 'Target_glottal',\n",
       " 'Target_stop',\n",
       " 'Target_fricative',\n",
       " 'Target_affricate',\n",
       " 'Target_glide',\n",
       " 'Target_Place',\n",
       " 'Target_Manner',\n",
       " 'Target_Height',\n",
       " 'Target_Frontness',\n",
       " 'Phon_Acc',\n",
       " 'Voicing_Acc',\n",
       " 'Place_Acc',\n",
       " 'Manner_Acc',\n",
       " 'Height_Acc',\n",
       " 'Frontness_Acc',\n",
       " 'Tenseness_Acc',\n",
       " 'Roundness_Acc',\n",
       " 'wab1_aq',\n",
       " 'wab1_nwf_total',\n",
       " 'Session_Type',\n",
       " 'wabaq_start',\n",
       " 'Target_Word_IPA',\n",
       " 'Prod_Word_IPA',\n",
       " 'Prod_N_Tot_Phonemes',\n",
       " 'Prod_Phoneme_IPA',\n",
       " 'Target_Phoneme_IPA',\n",
       " 'Damerau_Levenshtein']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phon_Sess_Code</th>\n",
       "      <th>syllabic_Acc</th>\n",
       "      <th>consonantal_Acc</th>\n",
       "      <th>sonorant_Acc</th>\n",
       "      <th>continuant_Acc</th>\n",
       "      <th>delayed release_Acc</th>\n",
       "      <th>approximant_Acc</th>\n",
       "      <th>tap_Acc</th>\n",
       "      <th>nasal_Acc</th>\n",
       "      <th>voice_Acc</th>\n",
       "      <th>...</th>\n",
       "      <th>post-alveolar_Acc</th>\n",
       "      <th>dental_Acc</th>\n",
       "      <th>palatal_Acc</th>\n",
       "      <th>glottal_Acc</th>\n",
       "      <th>stop_Acc</th>\n",
       "      <th>fricative_Acc</th>\n",
       "      <th>affricate_Acc</th>\n",
       "      <th>glide_Acc</th>\n",
       "      <th>FeatureWeighted_PhonAcc</th>\n",
       "      <th>PVMWeighted_PhonAcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15_0_1_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15_0_1_2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15_0_1_3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15_0_2_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15_0_2_2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Phon_Sess_Code  syllabic_Acc  consonantal_Acc  sonorant_Acc  continuant_Acc  \\\n",
       "0       15_0_1_1             1                1             1               1   \n",
       "1       15_0_1_2             1                1             1               1   \n",
       "2       15_0_1_3             1                1             1               1   \n",
       "3       15_0_2_1             1                1             1               1   \n",
       "4       15_0_2_2             1                1             1               1   \n",
       "\n",
       "   delayed release_Acc  approximant_Acc  tap_Acc  nasal_Acc  voice_Acc  ...  \\\n",
       "0                    1                1        1          1          1  ...   \n",
       "1                    1                1        1          1          1  ...   \n",
       "2                    1                1        1          1          1  ...   \n",
       "3                    1                1        1          1          1  ...   \n",
       "4                    1                1        1          1          1  ...   \n",
       "\n",
       "   post-alveolar_Acc  dental_Acc  palatal_Acc  glottal_Acc  stop_Acc  \\\n",
       "0                  1           1            1            1         1   \n",
       "1                  1           1            1            1         1   \n",
       "2                  1           1            1            1         1   \n",
       "3                  1           1            1            1         1   \n",
       "4                  1           1            1            1         1   \n",
       "\n",
       "   fricative_Acc  affricate_Acc  glide_Acc  FeatureWeighted_PhonAcc  \\\n",
       "0              1              1          1                      1.0   \n",
       "1              1              1          1                      1.0   \n",
       "2              1              1          1                      1.0   \n",
       "3              1              1          1                      1.0   \n",
       "4              1              1          1                      1.0   \n",
       "\n",
       "   PVMWeighted_PhonAcc  \n",
       "0                  1.0  \n",
       "1                  1.0  \n",
       "2                  1.0  \n",
       "3                  1.0  \n",
       "4                  1.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AccDat = pd.read_csv(\"Resources/AllAccScores_080123.csv\")\n",
    "AccDat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Production</th>\n",
       "      <th>Prod_Word_Dur</th>\n",
       "      <th>Prod_Arpabet</th>\n",
       "      <th>Prod_Phon_Dur</th>\n",
       "      <th>NOTES</th>\n",
       "      <th>Word_ID</th>\n",
       "      <th>Session_ID</th>\n",
       "      <th>Prod_Word_N</th>\n",
       "      <th>...</th>\n",
       "      <th>post-alveolar_Acc</th>\n",
       "      <th>dental_Acc</th>\n",
       "      <th>palatal_Acc</th>\n",
       "      <th>glottal_Acc</th>\n",
       "      <th>stop_Acc</th>\n",
       "      <th>fricative_Acc</th>\n",
       "      <th>affricate_Acc</th>\n",
       "      <th>glide_Acc</th>\n",
       "      <th>FeatureWeighted_PhonAcc</th>\n",
       "      <th>PVMWeighted_PhonAcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>B</td>\n",
       "      <td>0.024363</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>UH</td>\n",
       "      <td>0.163408</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>book</td>\n",
       "      <td>B UH K</td>\n",
       "      <td>0.295646</td>\n",
       "      <td>K</td>\n",
       "      <td>0.107875</td>\n",
       "      <td>Article (ÃÂ) before word</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>ball</td>\n",
       "      <td>B AO L</td>\n",
       "      <td>0.397365</td>\n",
       "      <td>B</td>\n",
       "      <td>0.014197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>ball</td>\n",
       "      <td>B AO L</td>\n",
       "      <td>0.397365</td>\n",
       "      <td>AO</td>\n",
       "      <td>0.211006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PID Target Production  Prod_Word_Dur Prod_Arpabet  Prod_Phon_Dur  \\\n",
       "0   15   book     B UH K       0.295646            B       0.024363   \n",
       "1   15   book     B UH K       0.295646           UH       0.163408   \n",
       "2   15   book     B UH K       0.295646            K       0.107875   \n",
       "3   15   ball     B AO L       0.397365            B       0.014197   \n",
       "4   15   ball     B AO L       0.397365           AO       0.211006   \n",
       "\n",
       "                        NOTES  Word_ID  Session_ID  Prod_Word_N  ...  \\\n",
       "0  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "1  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "2  Article (ÃÂ) before word      1.0         0.0          1.0  ...   \n",
       "3                         NaN      2.0         0.0          2.0  ...   \n",
       "4                         NaN      2.0         0.0          2.0  ...   \n",
       "\n",
       "   post-alveolar_Acc dental_Acc palatal_Acc glottal_Acc  stop_Acc  \\\n",
       "0                  1          1           1           1         1   \n",
       "1                  1          1           1           1         1   \n",
       "2                  1          1           1           1         1   \n",
       "3                  1          1           1           1         1   \n",
       "4                  1          1           1           1         1   \n",
       "\n",
       "   fricative_Acc affricate_Acc  glide_Acc  FeatureWeighted_PhonAcc  \\\n",
       "0              1             1          1                      1.0   \n",
       "1              1             1          1                      1.0   \n",
       "2              1             1          1                      1.0   \n",
       "3              1             1          1                      1.0   \n",
       "4              1             1          1                      1.0   \n",
       "\n",
       "   PVMWeighted_PhonAcc  \n",
       "0                  1.0  \n",
       "1                  1.0  \n",
       "2                  1.0  \n",
       "3                  1.0  \n",
       "4                  1.0  \n",
       "\n",
       "[5 rows x 181 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.merge(AccDat, on='Phon_Sess_Code', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PID',\n",
       " 'Target',\n",
       " 'Production',\n",
       " 'Prod_Word_Dur',\n",
       " 'Prod_Arpabet',\n",
       " 'Prod_Phon_Dur',\n",
       " 'NOTES',\n",
       " 'Word_ID',\n",
       " 'Session_ID',\n",
       " 'Prod_Word_N',\n",
       " 'Prod_Phon_N',\n",
       " 'Code',\n",
       " 'Phon_Sess_Code',\n",
       " 'Word_Sess_Code',\n",
       " 'Prod_Last_Phon',\n",
       " 'Prod_Phoneme_ID',\n",
       " 'Target_Arpabet',\n",
       " 'Target_N_Tot_Words',\n",
       " 'Target_N_Tot_Syllables',\n",
       " 'Target_N_Tot_Phonemes',\n",
       " 'Target_Phon_Arpabet',\n",
       " 'Target_Phoneme_ID',\n",
       " 'Target_Syll_Env',\n",
       " 'Target_Word_Pos',\n",
       " 'Syllable_NumID',\n",
       " 'Target_Word_NumID',\n",
       " 'Target_Con_Cluster',\n",
       " 'Target_Clust_ID',\n",
       " 'Target_Clus_Type',\n",
       " 'Target_Clust_Phon_Pos',\n",
       " 'Target_Clust_Phon_Env',\n",
       " 'Target_Prev_Phon',\n",
       " 'Target_Next_Phon',\n",
       " 'Prod_syllabic',\n",
       " 'Prod_consonantal',\n",
       " 'Prod_sonorant',\n",
       " 'Prod_continuant',\n",
       " 'Prod_delayed release',\n",
       " 'Prod_approximant',\n",
       " 'Prod_tap',\n",
       " 'Prod_nasal',\n",
       " 'Prod_voice',\n",
       " 'Prod_spread gl',\n",
       " 'Prod_constr gl',\n",
       " 'Prod_labial',\n",
       " 'Prod_round',\n",
       " 'Prod_labiodental',\n",
       " 'Prod_coronal',\n",
       " 'Prod_anterior',\n",
       " 'Prod_distributed',\n",
       " 'Prod_strident',\n",
       " 'Prod_lateral',\n",
       " 'Prod_dorsal',\n",
       " 'Prod_high',\n",
       " 'Prod_low',\n",
       " 'Prod_front',\n",
       " 'Prod_back',\n",
       " 'Prod_tense',\n",
       " 'Prod_lax',\n",
       " 'Prod_vowel',\n",
       " 'Prod_consonant',\n",
       " 'Prod_diphthong',\n",
       " 'Prod_monophthong',\n",
       " 'Prod_velar',\n",
       " 'Prod_alveolar',\n",
       " 'Prod_post-alveolar',\n",
       " 'Prod_dental',\n",
       " 'Prod_palatal',\n",
       " 'Prod_glottal',\n",
       " 'Prod_stop',\n",
       " 'Prod_fricative',\n",
       " 'Prod_affricate',\n",
       " 'Prod_glide',\n",
       " 'Prod_Place',\n",
       " 'Prod_Manner',\n",
       " 'Prod_Height',\n",
       " 'Prod_Frontness',\n",
       " 'Target_syllabic',\n",
       " 'Target_consonantal',\n",
       " 'Target_sonorant',\n",
       " 'Target_continuant',\n",
       " 'Target_delayed release',\n",
       " 'Target_approximant',\n",
       " 'Target_tap',\n",
       " 'Target_nasal',\n",
       " 'Target_voice',\n",
       " 'Target_spread gl',\n",
       " 'Target_constr gl',\n",
       " 'Target_labial',\n",
       " 'Target_round',\n",
       " 'Target_labiodental',\n",
       " 'Target_coronal',\n",
       " 'Target_anterior',\n",
       " 'Target_distributed',\n",
       " 'Target_strident',\n",
       " 'Target_lateral',\n",
       " 'Target_dorsal',\n",
       " 'Target_high',\n",
       " 'Target_low',\n",
       " 'Target_front',\n",
       " 'Target_back',\n",
       " 'Target_tense',\n",
       " 'Target_lax',\n",
       " 'Target_vowel',\n",
       " 'Target_consonant',\n",
       " 'Target_diphthong',\n",
       " 'Target_monophthong',\n",
       " 'Target_velar',\n",
       " 'Target_alveolar',\n",
       " 'Target_post-alveolar',\n",
       " 'Target_dental',\n",
       " 'Target_palatal',\n",
       " 'Target_glottal',\n",
       " 'Target_stop',\n",
       " 'Target_fricative',\n",
       " 'Target_affricate',\n",
       " 'Target_glide',\n",
       " 'Target_Place',\n",
       " 'Target_Manner',\n",
       " 'Target_Height',\n",
       " 'Target_Frontness',\n",
       " 'Phon_Acc',\n",
       " 'Voicing_Acc',\n",
       " 'Place_Acc',\n",
       " 'Manner_Acc',\n",
       " 'Height_Acc',\n",
       " 'Frontness_Acc',\n",
       " 'Tenseness_Acc',\n",
       " 'Roundness_Acc',\n",
       " 'wab1_aq',\n",
       " 'wab1_nwf_total',\n",
       " 'Session_Type',\n",
       " 'wabaq_start',\n",
       " 'Target_Word_IPA',\n",
       " 'Prod_Word_IPA',\n",
       " 'Prod_N_Tot_Phonemes',\n",
       " 'Prod_Phoneme_IPA',\n",
       " 'Target_Phoneme_IPA',\n",
       " 'Damerau_Levenshtein',\n",
       " 'syllabic_Acc',\n",
       " 'consonantal_Acc',\n",
       " 'sonorant_Acc',\n",
       " 'continuant_Acc',\n",
       " 'delayed release_Acc',\n",
       " 'approximant_Acc',\n",
       " 'tap_Acc',\n",
       " 'nasal_Acc',\n",
       " 'voice_Acc',\n",
       " 'spread gl_Acc',\n",
       " 'constr gl_Acc',\n",
       " 'labial_Acc',\n",
       " 'round_Acc',\n",
       " 'labiodental_Acc',\n",
       " 'coronal_Acc',\n",
       " 'anterior_Acc',\n",
       " 'distributed_Acc',\n",
       " 'strident_Acc',\n",
       " 'lateral_Acc',\n",
       " 'dorsal_Acc',\n",
       " 'high_Acc',\n",
       " 'low_Acc',\n",
       " 'front_Acc',\n",
       " 'back_Acc',\n",
       " 'tense_Acc',\n",
       " 'lax_Acc',\n",
       " 'vowel_Acc',\n",
       " 'consonant_Acc',\n",
       " 'diphthong_Acc',\n",
       " 'monophthong_Acc',\n",
       " 'velar_Acc',\n",
       " 'alveolar_Acc',\n",
       " 'post-alveolar_Acc',\n",
       " 'dental_Acc',\n",
       " 'palatal_Acc',\n",
       " 'glottal_Acc',\n",
       " 'stop_Acc',\n",
       " 'fricative_Acc',\n",
       " 'affricate_Acc',\n",
       " 'glide_Acc',\n",
       " 'FeatureWeighted_PhonAcc',\n",
       " 'PVMWeighted_PhonAcc']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(df['Target_N_Tot_Phonemes'].unique()))\n",
    "print(sorted(df['Damerau_Levenshtein'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phon_Sess_Code</th>\n",
       "      <th>Target_Word_IPA</th>\n",
       "      <th>Prod_Word_IPA</th>\n",
       "      <th>Target_Phoneme_IPA</th>\n",
       "      <th>Prod_Phoneme_IPA</th>\n",
       "      <th>Prod_N_Tot_Phonemes</th>\n",
       "      <th>Damerau_Levenshtein</th>\n",
       "      <th>syllabic_Acc</th>\n",
       "      <th>consonantal_Acc</th>\n",
       "      <th>sonorant_Acc</th>\n",
       "      <th>...</th>\n",
       "      <th>post-alveolar_Acc</th>\n",
       "      <th>dental_Acc</th>\n",
       "      <th>palatal_Acc</th>\n",
       "      <th>glottal_Acc</th>\n",
       "      <th>stop_Acc</th>\n",
       "      <th>fricative_Acc</th>\n",
       "      <th>affricate_Acc</th>\n",
       "      <th>glide_Acc</th>\n",
       "      <th>FeatureWeighted_PhonAcc</th>\n",
       "      <th>PVMWeighted_PhonAcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15_0_1_1</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15_0_1_2</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>ʊ</td>\n",
       "      <td>ʊ</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15_0_1_3</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>bʊk</td>\n",
       "      <td>k</td>\n",
       "      <td>k</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15_0_2_1</td>\n",
       "      <td>bɔl</td>\n",
       "      <td>bɔl</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15_0_2_2</td>\n",
       "      <td>bɔl</td>\n",
       "      <td>bɔl</td>\n",
       "      <td>ɔ</td>\n",
       "      <td>ɔ</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Phon_Sess_Code Target_Word_IPA Prod_Word_IPA Target_Phoneme_IPA  \\\n",
       "0       15_0_1_1             bʊk           bʊk                  b   \n",
       "1       15_0_1_2             bʊk           bʊk                  ʊ   \n",
       "2       15_0_1_3             bʊk           bʊk                  k   \n",
       "3       15_0_2_1             bɔl           bɔl                  b   \n",
       "4       15_0_2_2             bɔl           bɔl                  ɔ   \n",
       "\n",
       "  Prod_Phoneme_IPA  Prod_N_Tot_Phonemes  Damerau_Levenshtein  syllabic_Acc  \\\n",
       "0                b                    3                    0             1   \n",
       "1                ʊ                    3                    0             1   \n",
       "2                k                    3                    0             1   \n",
       "3                b                    3                    0             1   \n",
       "4                ɔ                    3                    0             1   \n",
       "\n",
       "   consonantal_Acc  sonorant_Acc  ...  post-alveolar_Acc  dental_Acc  \\\n",
       "0                1             1  ...                  1           1   \n",
       "1                1             1  ...                  1           1   \n",
       "2                1             1  ...                  1           1   \n",
       "3                1             1  ...                  1           1   \n",
       "4                1             1  ...                  1           1   \n",
       "\n",
       "   palatal_Acc  glottal_Acc  stop_Acc  fricative_Acc  affricate_Acc  \\\n",
       "0            1            1         1              1              1   \n",
       "1            1            1         1              1              1   \n",
       "2            1            1         1              1              1   \n",
       "3            1            1         1              1              1   \n",
       "4            1            1         1              1              1   \n",
       "\n",
       "   glide_Acc  FeatureWeighted_PhonAcc  PVMWeighted_PhonAcc  \n",
       "0          1                      1.0                  1.0  \n",
       "1          1                      1.0                  1.0  \n",
       "2          1                      1.0                  1.0  \n",
       "3          1                      1.0                  1.0  \n",
       "4          1                      1.0                  1.0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df[[\n",
    "    'Phon_Sess_Code',\n",
    "    'Target_Word_IPA',\n",
    "    'Prod_Word_IPA',\n",
    "    'Target_Phoneme_IPA',\n",
    "    'Prod_Phoneme_IPA',\n",
    "    'Prod_N_Tot_Phonemes',\n",
    "    'Damerau_Levenshtein',\n",
    "    'syllabic_Acc',\n",
    "    'consonantal_Acc',\n",
    "    'sonorant_Acc',\n",
    "    'continuant_Acc',\n",
    "    'delayed release_Acc',\n",
    "    'approximant_Acc',\n",
    "    'tap_Acc',\n",
    "    'nasal_Acc',\n",
    "    'voice_Acc',\n",
    "    'spread gl_Acc',\n",
    "    'constr gl_Acc',\n",
    "    'labial_Acc',\n",
    "    'round_Acc',\n",
    "    'labiodental_Acc',\n",
    "    'coronal_Acc',\n",
    "    'anterior_Acc',\n",
    "    'distributed_Acc',\n",
    "    'strident_Acc',\n",
    "    'lateral_Acc',\n",
    "    'dorsal_Acc',\n",
    "    'high_Acc',\n",
    "    'low_Acc',\n",
    "    'front_Acc',\n",
    "    'back_Acc',\n",
    "    'tense_Acc',\n",
    "    'lax_Acc',\n",
    "    'vowel_Acc',\n",
    "    'consonant_Acc',\n",
    "    'diphthong_Acc',\n",
    "    'monophthong_Acc',\n",
    "    'velar_Acc',\n",
    "    'alveolar_Acc',\n",
    "    'post-alveolar_Acc',\n",
    "    'dental_Acc',\n",
    "    'palatal_Acc',\n",
    "    'glottal_Acc',\n",
    "    'stop_Acc',\n",
    "    'fricative_Acc',\n",
    "    'affricate_Acc',\n",
    "    'glide_Acc',\n",
    "    'FeatureWeighted_PhonAcc',\n",
    "    'PVMWeighted_PhonAcc'\n",
    "]].copy()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Phon_Sess_Code',\n",
       " 'Target_Word_IPA',\n",
       " 'Prod_Word_IPA',\n",
       " 'Target_Phoneme_IPA',\n",
       " 'Prod_Phoneme_IPA',\n",
       " 'Prod_N_Tot_Phonemes',\n",
       " 'Damerau_Levenshtein',\n",
       " 'syllabic_Acc',\n",
       " 'consonantal_Acc',\n",
       " 'sonorant_Acc',\n",
       " 'continuant_Acc',\n",
       " 'delayed release_Acc',\n",
       " 'approximant_Acc',\n",
       " 'tap_Acc',\n",
       " 'nasal_Acc',\n",
       " 'voice_Acc',\n",
       " 'spread gl_Acc',\n",
       " 'constr gl_Acc',\n",
       " 'labial_Acc',\n",
       " 'round_Acc',\n",
       " 'labiodental_Acc',\n",
       " 'coronal_Acc',\n",
       " 'anterior_Acc',\n",
       " 'distributed_Acc',\n",
       " 'strident_Acc',\n",
       " 'lateral_Acc',\n",
       " 'dorsal_Acc',\n",
       " 'high_Acc',\n",
       " 'low_Acc',\n",
       " 'front_Acc',\n",
       " 'back_Acc',\n",
       " 'tense_Acc',\n",
       " 'lax_Acc',\n",
       " 'vowel_Acc',\n",
       " 'consonant_Acc',\n",
       " 'diphthong_Acc',\n",
       " 'monophthong_Acc',\n",
       " 'velar_Acc',\n",
       " 'alveolar_Acc',\n",
       " 'post-alveolar_Acc',\n",
       " 'dental_Acc',\n",
       " 'palatal_Acc',\n",
       " 'glottal_Acc',\n",
       " 'stop_Acc',\n",
       " 'fricative_Acc',\n",
       " 'affricate_Acc',\n",
       " 'glide_Acc',\n",
       " 'FeatureWeighted_PhonAcc',\n",
       " 'PVMWeighted_PhonAcc']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.110030395136778"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.Damerau_Levenshtein.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['61-70', '91-100', '71-80', '41-50', '81-90'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.wabaq_start.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Resources/all_data_pvm_acc5.csv', index=False)\n",
    "df2.to_csv('Resources/AllAccScores_DamLev_080123.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
